# 가상 메모리

기존에는 프로세스가 실행되는 코드의 전체를 메모리에 로드해야 했고, 메모리 요양보다 더 큰 프로그램은 실행시킬 수 없었다. 하지만 실제로는 코드의 일부에서만 대부분의 시간을 사용하고, 프로세스는 특정 순간에는 항상 작은 양의 주소 공간을 사용했기 때문에 이러한 방식은 매우 비효율적이었다

가상 메모리는(Virtual Memory)는 이러한 **물리적 메모리 크기의 한계**를 극복하기 위해 나온 기술이다. 프로세스를 실행할 때 실행에 필요한 일부만 메모리에 로드하고 나머지는 디스크에 두는 것이다. 이를 통해 프로세스 전체가 물리적 메모리에 있는 것'처럼' 수행되는, 즉 물리적 메모리가 훨씬 많이 있는 것처럼 보이게 된다. 결과적으로 메모리에 작은 양의 주소 공간만 있으면 충분히 프로세스를 수행할 수 있고, 그에 따라 더 많은 프로그램을 동시에 실행할 수 있게 된다.

**프로그램의 일부분만 메모리에 올릴 수 있다면**

* 물리 메모리 크기에 제약받지 않게 된다.
* 더 많은 프로그램을 동시에 실행할 수 있게 된다. 이에 따라 `응답시간`은 유지되고, `CPU 이용률`과 `처리율`은 높아진다.
* swap에 필요한 입출력이 줄어들기 때문에 프로그램이 빠르게 실행된다.

<br/>

## 가상 메모리가 하는 일

가상 메모리는 실제의 물리 메모리 개념과 사용자의 논리 메모리 개념을 분리한 것으로 정리할 수 있다. 이로써 작은 메모리를 가지고도 얼마든지 큰 `가상 주소 공간`을 프로그래머에게 제공할 수 있다.

<br/>

### 가상 주소 공간

* 한 프로세스가 메모리에 저장되는 논리적인 모습을 가상메모리에 구현한 공간이다. 프로세스가 요구하는 메모리 공간을 가상메모리에서 제공함으로써 현재 직접적으로 필요치 않은 메모리 공간은 실제 물리 메모리에 올리지 않는 것으로 물리 메모리를 절약할 수 있다.
* 예를 들어, 한 프로그램이 실행되며 논리 메모리로 100KB가 요구되었다고 하자. 하지만 실행까지에 필요한 메모리 공간 `(Heap 영역, Stack 영역, 코드, 데이터)`의 합이 40KB 라면, 실제 물리 메모리에는 40KB 만 올라가 있고, 나머지 60KB 만큼은 필요시에 물리메모리에 요구한다고 이해할 수 있겠다.

| `Stack` | `free (60KB)` | `Heap` | `Data` | `Code` |
|:---:|:---:|:---:|:---:|:---:|

<br/>

### 프로세스간의 페이지 공유

가상 메모리는...

* `시스템 라이브러리`가 여러 프로세스들 사이에 공유될 수 있도록 한다. 각 프로세스들은 `공유 라이브러리`를 자신의 가상 주소 공간에 두고 사용하는 것처럼 인식하지만, 라이브러리가 올라가 있는 `물리 메모리 페이지`들은 모든 프로세스에 공유되고 있다.
* 프로세스들이 메모리를 공유하는 것을 가능하게 하고, 프로세스들은 공유 메모리를 통해 통신할 수 있다. 이 또한, 각 프로세스들은 각자 자신의 주소 공간처럼 인식하지만, 실제 물리 메모리는 공유되고 있다.
* `fork()`를 통한 프로세스 생성 과정에서 페이지들이 공유되는 것을 가능하게 한다.

<br/>

## Demand Paging(요구 페이징)

프로그램 실행 시작 시에 프로그램 전체를 디스크에서 물리 메모리에 적재하는 대신, 초기에 필요한 것들만 적재하는 전략을 `요구 페이징`이라 하며, 가상 메모리 시스템에서 많이 사용된다. 그리고 가상 메모리는 대개 페이지로 관리된다. 요구 페이징을 사용하는 가상 메모리에서는 실행과정에서 필요해질 때 페이지들이 적재된다. **한 번도 접근되지 않은 페이지는 물리 메모리에 적재되지 않는다.**

프로세스 내의 개별 페이지들은 `페이저(pager)`에 의해 관리된다. 페이저는 프로세스 실행에 실제 필요한 페이지들만 메모리로 읽어 옮으로써, **사용되지 않을 페이지를 가져오는 시간낭비와 메모리 낭비를 줄일 수 있다.**

**Page falut trap(페이지 부재 트랩)**

<br/>

## 페이지 교체

`요구 페이징` 에서 언급된대로 프로그램 실행시에 모든 항목이 물리 메모리에 올라오지 않기 때문에, 프로세스의 동작에 필요한 페이지를 요청하는 과정에서 `page fault(페이지 부재)`가 발생하게 되면, 원하는 페이지를 보조저장장치에서 가져오게 된다. 하지만, 만약 물리 메모리가 모두 사용중인 상황이라면, 페이지 교체가 이뤄줘야 한다(또는, 운영체제가 프로세스를 강제 종료하는 방법이 있다).

<br/>

### 기본적인 방법

물리 메모리가 모두 사용중인 상황에서의 메모리 교체 흐름이다.

1. 디스크에서 필요한 페이지의 위치를 찾는다.
2. 빈 페이지 프레임을 찾는다.
    1. `페이지 교체 알고리즘`을 통해 희생될(victim) 페이지를 고른다.
    2. 희생될 페이지를 디스크에 기록하고, 관련 페이지 테이블을 수정한다.
3. 새롭게 비워진 페이지 테이블 내 프레임에 새 페이지를 읽어오고, 프레임 테이블을 수정한다.
4. 사용자 프로세스 재시작

<br/>

### 페이지 교체 알고리즘

1. **FIFO 페이지 교체**

가장 간단한 페이지 교체 알고리즘으로 FIFO(first-in first-out)의 흐름을 가진다. 즉, 먼저 물리 메모리에 들어온 페이지 순서대로 페이지 교체 시점에 먼저 나가게 된다는 것이다.

* 장점
    * 이해하기도 쉽고, 프로그램하기도 쉽다.
* 단점
    * 오래된 페이지가 항상 불필요하지 않은 정보를 포함하지 않을 수 있다(초기 변수 등)
    * 처음부터 활발하게 사용되는 페이지를 교체해서 페이지 부재율을 높이는 부작용을 초래할 수 있다.
    * `Belady의 모순` : 페이지를 저장할 수 있는 페이지 프레임의 갯수를 늘려도 되려 페이지 부재가 더 많이 발생하는 모순이 존재한다.

<br/>

2. **최적 페이지 교체(Optimal Page Replacement)**

`Belady의 모순`을 확인한 이후 최적 교체 알고리즘에 대한 탐구가 진행되었고, 모든 알고리즘보다 낮은 페이지 부재율을 보이며 `Belady의 모순`이 발생하지 않는다. 이 알고리즘의 핵심은 `앞으로 가장 오랫동안 사용되지 않을 페이지를 찾아 교체`하는 것이다. 주로 비교 연구 목적을 위해 사용한다.

* 장점
    * 알고리즘 중 가장 낮은 페이지 부재율을 보장한다.
* 단점
    * 구현의 어려움이 있다. 모든 프로세스의 메모리 참조의 계획을 미리 파악할 방법이 없기 때문이다.

<br/>

3. **LRU 페이지 교체(LRU Page Replacement)**

`LRU(Least-Recently-Used)` 알고리즘은 **가장 오래전에 참조된 것을 지우는 방법** 이다. Optimal에 근접한 방법이며, Belady's anomaly가 발생하지 않는다. 다만, 구현하기가 어렵고 접근되는 빈도를 고려하지 않는다는 단점이 있다.

**연결 리스트**로 LRU를 구현하면 **O(1)**만에 page를 찾고 삽입할 수 있다. 제일 최근에 참조된 page를 가장 앞으로 옮기는 방식으로 연결 리스트를 구현하면 replace가 일어날 때 가장 뒤에 있는 page를 바꿔주면 된다.

* 특징
    * 대체적으로 `FIFO 알고리즘` 보다 우수하고, `OPT 알고리즘` 보다는 그렇지 못한 모습을 보인다.

<br/>

4. **LFU 페이지 교체(LFU Page Replacement)**

`LFU(Least-Frequently-Used)`는 참조 횟수가 가장 적은 페이지를 교체하는 방법이다. 활발하게 사용되는 페이지는 참조 횟수가 많아질 거라는 가정에서 만들어진 알고리즘이다. LRU에 비해 장지적인 시간 규모를 보기 떄문에 page의 인기도를 조금 더 정확히 반영할 수 있다. 하지만 최근성은 반영하지 못한다. 최저 참조 횟수인 page가 2개 이상인 경우에는 LFU 알고리즘 자체에서는 임의로 page를 선정하는데, 성능 향상을 위해 가장 오래전에 참조된 page를 지우는 식으로 구현할 수도 있다.

LFU를 LRU처럼 연결 리스트를 이용해서 구현하면 replace 될 page를 찾는데 **O(n)**의 시간이 걸리게 되어 느리다. 따라서 **힙(heap)**을 사용하면 최소 빈도를 갖는 page를 찾거나 삽입, 삭제하는데 **O(logn)**의 사긴이 걸리게 되므로 훨씬 효율적이다.

* 특징
    * 어떤 프로세스가 특정 페이지를 집중적으로 사용하다가 다른 기능을 사용하게되면 더 이상 사용하지 않아도 계속 메모리에 머물게 되어 초기 가정에 어긋나는 시점이 발생할 수 있다.
    * 최적(OPT) 페이지 교체를 제대로 근사하지 못하기 때문에, 잘 쓰이지 않는다.

<br/>

5. **Second Chance ALgorithm(Clock Algorithm)**

LRU와 LFU 알고리즘이 실제로 paging system에서 사용 가능할까? 그렇지 않다. LRU와 LFU의 알고리즘 구현에서 운영체제가 자료구조를 변겨앟고 유지하는 작업을 수행해야 하는데, 이미 메모리에 page가 올라가 있는 경우에는 CPU가 운영체제에 넘어가지 않는다. page fault가 발생해야 CPU가 운영체제에 넘어가고, 디스크에서 메모리로 page를 로드할 때 해당 page에 대한 정보를 얻고 갱신할 수 있다. 따라서 운영체제가 참조한 지 오래되거나 참조 횟수가 적은 페이지를 정확하게 알 수 없다.

이에 대한 해결법으로 Second Chance ALgorithm을 사용한다. **Clock Algorithm** 이라고도 불린다.

**Secnod Chance Algorithm은 LRU의 근사(approximation) 알고리즘으로, 최근에 참조되었는지 여부를 나타내는 Reference bit이라는 정보를 사용한다.**

Reference bit가 0인 것을 찾을 때까지 시계처럼 한 바퀴씩 포인터를 이동하다가 0인 것을 찾으면 해당 page를 교체하는 방식이다. 만약 Reference bit가 1인 page를 만나면 0으로 바꿔주고, 한 바퀴 되돌아와서도(Second Chance) 여전히 0이면 해당 page를 교체한다. 다시 bit가 1로 바뀌었다면 그만큼 자주 사용되는 page라는 의미인 것이다.

![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FTq6lT%2FbtrgfN6QDcX%2FQ4SKeSJ78I9JEXEYH74KH1%2Fimg.png)

이를 조금 더 개선한 방식으로 Enhanced Second Change Algorithm이 있는데, 최근에 해당 page가 변경되었는지를 나타내는 **Modified bit(dirty bit)** 가 추가된 것이다.

Modified bit가 1이라면 page가 변경되었기 땜누에 교체를 하면 디스크에 해당 내용을 반영해야 한다. 즉, I/O 작업이 동반되므로 시간이 오래 걸린다. 따라서 "Reference bit == 0? -> Modified bit == 0?" 순서로 우선순위를 가진다.

<br/>

**참조**

* [원본링크](https://github.com/JaeYeopHan/Interview_Question_for_Beginner/tree/master/OS#%EA%B0%80%EC%83%81-%EB%A9%94%EB%AA%A8%EB%A6%AC)
* [자세한 내용](https://rebro.kr/179?category=504670)